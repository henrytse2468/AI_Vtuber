{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare filelists for LJSpeech dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://github.com/espeak-ng/espeak-ng/blob/master/docs/languages.md\n",
    "dir_data = \"/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/output_folder\"\n",
    "config = \"../config.yaml\"\n",
    "symlink = \"DUMMY3\"\n",
    "n_val = 100\n",
    "n_test = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'PHONEMIZER_ESPEAK_LIBRARY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpathlib\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(pathlib\u001b[39m.\u001b[39mPath(os\u001b[39m.\u001b[39;49menviron[\u001b[39m'\u001b[39;49m\u001b[39mPHONEMIZER_ESPEAK_LIBRARY\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Output: PosixPath('/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mctypes\u001b[39;00m\n",
      "File \u001b[0;32m<frozen os>:679\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'PHONEMIZER_ESPEAK_LIBRARY'"
     ]
    }
   ],
   "source": [
    "!export PHONEMIZER_ESPEAK_LIBRARY=/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.dylibimport os\n",
    "import pathlib\n",
    "print(pathlib.Path(os.environ['PHONEMIZER_ESPEAK_LIBRARY']))\n",
    "# Output: PosixPath('/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib')\n",
    "\n",
    "import ctypes\n",
    "print(ctypes.cdll.LoadLibrary('/usr/local/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib'))\n",
    "# Output: <CDLL '/opt/homebrew/Cellar/espeak/1.48.04_1/lib/libespeak.1.1.48.dylib', handle 89a128f0 at 0x10b42ccd0>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hyperparameters from config file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.hparams import get_hparams_from_file\n",
    "\n",
    "hps = get_hparams_from_file(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset\n",
    "\n",
    "Here `normalized_text` contains numbers in the form of words.\n",
    "\n",
    "**Note**: you may need to replace all `\"|\"` with `\" | \"` in the file `metadata.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DUMMY3/HololiveTest1.wav</td>\n",
       "      <td>Oh, wouldn't it be ironicif IRyS sabotages the...</td>\n",
       "      <td>Oh, wouldn't it be ironicif IRyS sabotages the...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DUMMY3/HololiveTest1.wav</td>\n",
       "      <td>Wake up, IRyS!</td>\n",
       "      <td>Wake up, IRyS!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DUMMY3/HololiveTest1.wav</td>\n",
       "      <td>Time to ruin Christmas!</td>\n",
       "      <td>Time to ruin Christmas!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DUMMY3/HololiveTest1.wav</td>\n",
       "      <td>Let's gooo!</td>\n",
       "      <td>Let's gooo!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DUMMY3/HololiveTest1.wav</td>\n",
       "      <td>How, how is that sabotaged?</td>\n",
       "      <td>How, how is that sabotaged?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file  \\\n",
       "0  DUMMY3/HololiveTest1.wav   \n",
       "1  DUMMY3/HololiveTest1.wav   \n",
       "2  DUMMY3/HololiveTest1.wav   \n",
       "3  DUMMY3/HololiveTest1.wav   \n",
       "4  DUMMY3/HololiveTest1.wav   \n",
       "\n",
       "                                                text  \\\n",
       "0  Oh, wouldn't it be ironicif IRyS sabotages the...   \n",
       "1                                     Wake up, IRyS!   \n",
       "2                            Time to ruin Christmas!   \n",
       "3                                        Let's gooo!   \n",
       "4                        How, how is that sabotaged?   \n",
       "\n",
       "                                     normalized_text  cleaned_text  \n",
       "0  Oh, wouldn't it be ironicif IRyS sabotages the...           NaN  \n",
       "1                                     Wake up, IRyS!           NaN  \n",
       "2                            Time to ruin Christmas!           NaN  \n",
       "3                                        Let's gooo!           NaN  \n",
       "4                        How, how is that sabotaged?           NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    f\"/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/train.csv\",\n",
    "    sep=r\"|\",\n",
    "    header=None,\n",
    "    names=[\"file\", \"text\", \"normalized_text\", \"cleaned_text\"],\n",
    "    index_col=False,\n",
    "    # converter to add .wav to file name\n",
    "    converters={\"file\": lambda x: f\"{symlink}/{x.strip()}.wav\", \"text\": str.strip, \"normalized_text\": str.strip},\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaners\n",
    "\n",
    "It may take a while, so better to preprocess the text and save it to a file in advance.\n",
    "\n",
    "**Note** `phonemize_text` takes the longest time.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenize_text', 'add_bos_eos']\n",
      "[['phonemize_text']]\n"
     ]
    }
   ],
   "source": [
    "# Get index of tokenize_text\n",
    "text_cleaners = hps.data.text_cleaners\n",
    "\n",
    "token_idx = text_cleaners.index(\"tokenize_text\")\n",
    "token_cleaners = text_cleaners[token_idx:]\n",
    "print(token_cleaners)\n",
    "\n",
    "\n",
    "# Extract phonemize_text\n",
    "def separate_text_cleaners(text_cleaners):\n",
    "    final_list = []\n",
    "    temp_list = []\n",
    "\n",
    "    for cleaner in text_cleaners:\n",
    "        if cleaner == \"phonemize_text\":\n",
    "            if temp_list:\n",
    "                final_list.append(temp_list)\n",
    "            final_list.append([cleaner])\n",
    "            temp_list = []\n",
    "        else:\n",
    "            temp_list.append(cleaner)\n",
    "\n",
    "    if temp_list:\n",
    "        final_list.append(temp_list)\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n",
    "text_cleaners = text_cleaners[:token_idx]\n",
    "text_cleaners = separate_text_cleaners(text_cleaners)\n",
    "print(text_cleaners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning with ['phonemize_text'] ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "espeak not installed on your system",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb Cell 10\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCleaning with \u001b[39m\u001b[39m{\u001b[39;00mcleaners\u001b[39m}\u001b[39;00m\u001b[39m ...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m cleaners[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mphonemize_text\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     text_norm \u001b[39m=\u001b[39m tokenizer(text_norm, Vocab, cleaners, language\u001b[39m=\u001b[39;49mhps\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mlanguage)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m idx, text \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(text_norm):\n",
      "File \u001b[0;32m~/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/text/__init__.py:17\u001b[0m, in \u001b[0;36mtokenizer\u001b[0;34m(text, vocab, cleaner_names, language, cleaned_text)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m    text: string to convert to a sequence of IDs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m    List of integers corresponding to the symbols in the text\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m cleaned_text:\n\u001b[0;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m _clean_text(text, vocab, cleaner_names, language\u001b[39m=\u001b[39;49mlanguage)\n\u001b[1;32m     18\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, text\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/text/__init__.py:31\u001b[0m, in \u001b[0;36m_clean_text\u001b[0;34m(text, vocab, cleaner_names, language)\u001b[0m\n\u001b[1;32m     29\u001b[0m     cleaner \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(cleaners, name)\n\u001b[1;32m     30\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mcallable\u001b[39m(cleaner), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown cleaner: \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m     text \u001b[39m=\u001b[39m cleaner(text, vocab\u001b[39m=\u001b[39;49mvocab, language\u001b[39m=\u001b[39;49mlanguage)\n\u001b[1;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m~/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/text/cleaners.py:41\u001b[0m, in \u001b[0;36mphonemize_text\u001b[0;34m(text, language, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mphonemize_text\u001b[39m(text: List[\u001b[39mstr\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39margs, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39men-us\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m phonemize(text, language\u001b[39m=\u001b[39;49mlanguage, backend\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mespeak\u001b[39;49m\u001b[39m\"\u001b[39;49m, separator\u001b[39m=\u001b[39;49mseparator, strip\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, preserve_punctuation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, punctuation_marks\u001b[39m=\u001b[39;49m_preserved_symbols_re, with_stress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, njobs\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vits2/lib/python3.11/site-packages/phonemizer/phonemize.py:206\u001b[0m, in \u001b[0;36mphonemize\u001b[0;34m(text, language, backend, separator, strip, prepend_text, preserve_empty_lines, preserve_punctuation, punctuation_marks, with_stress, tie, language_switch, words_mismatch, njobs, logger)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39m# initialize the phonemization backend\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m backend \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mespeak\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 206\u001b[0m     phonemizer \u001b[39m=\u001b[39m BACKENDS[backend](\n\u001b[1;32m    207\u001b[0m         language,\n\u001b[1;32m    208\u001b[0m         punctuation_marks\u001b[39m=\u001b[39;49mpunctuation_marks,\n\u001b[1;32m    209\u001b[0m         preserve_punctuation\u001b[39m=\u001b[39;49mpreserve_punctuation,\n\u001b[1;32m    210\u001b[0m         with_stress\u001b[39m=\u001b[39;49mwith_stress,\n\u001b[1;32m    211\u001b[0m         tie\u001b[39m=\u001b[39;49mtie,\n\u001b[1;32m    212\u001b[0m         language_switch\u001b[39m=\u001b[39;49mlanguage_switch,\n\u001b[1;32m    213\u001b[0m         words_mismatch\u001b[39m=\u001b[39;49mwords_mismatch,\n\u001b[1;32m    214\u001b[0m         logger\u001b[39m=\u001b[39;49mlogger)\n\u001b[1;32m    215\u001b[0m \u001b[39melif\u001b[39;00m backend \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mespeak-mbrola\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    216\u001b[0m     phonemizer \u001b[39m=\u001b[39m BACKENDS[backend](\n\u001b[1;32m    217\u001b[0m         language,\n\u001b[1;32m    218\u001b[0m         logger\u001b[39m=\u001b[39mlogger)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vits2/lib/python3.11/site-packages/phonemizer/backend/espeak/espeak.py:45\u001b[0m, in \u001b[0;36mEspeakBackend.__init__\u001b[0;34m(self, language, punctuation_marks, preserve_punctuation, with_stress, tie, language_switch, words_mismatch, logger)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, language: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     38\u001b[0m              punctuation_marks: Optional[Union[\u001b[39mstr\u001b[39m, Pattern]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     39\u001b[0m              preserve_punctuation: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m              words_mismatch: WordMismatch \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     44\u001b[0m              logger: Optional[Logger] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 45\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     46\u001b[0m         language, punctuation_marks\u001b[39m=\u001b[39;49mpunctuation_marks,\n\u001b[1;32m     47\u001b[0m         preserve_punctuation\u001b[39m=\u001b[39;49mpreserve_punctuation, logger\u001b[39m=\u001b[39;49mlogger)\n\u001b[1;32m     49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_espeak\u001b[39m.\u001b[39mset_voice(language)\n\u001b[1;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_with_stress \u001b[39m=\u001b[39m with_stress\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vits2/lib/python3.11/site-packages/phonemizer/backend/espeak/base.py:39\u001b[0m, in \u001b[0;36mBaseEspeakBackend.__init__\u001b[0;34m(self, language, punctuation_marks, preserve_punctuation, logger)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, language: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     36\u001b[0m              punctuation_marks: Optional[Union[\u001b[39mstr\u001b[39m, Pattern]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m              preserve_punctuation: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m              logger: Optional[Logger] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 39\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     40\u001b[0m         language,\n\u001b[1;32m     41\u001b[0m         punctuation_marks\u001b[39m=\u001b[39;49mpunctuation_marks,\n\u001b[1;32m     42\u001b[0m         preserve_punctuation\u001b[39m=\u001b[39;49mpreserve_punctuation,\n\u001b[1;32m     43\u001b[0m         logger\u001b[39m=\u001b[39;49mlogger)\n\u001b[1;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_espeak \u001b[39m=\u001b[39m EspeakWrapper()\n\u001b[1;32m     46\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mloaded \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_espeak\u001b[39m.\u001b[39mlibrary_path)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vits2/lib/python3.11/site-packages/phonemizer/backend/base.py:77\u001b[0m, in \u001b[0;36mBaseBackend.__init__\u001b[0;34m(self, language, punctuation_marks, preserve_punctuation, logger)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39m# ensure the backend is installed on the system\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(  \u001b[39m# pragma: nocover\u001b[39;00m\n\u001b[1;32m     78\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m not installed on your system\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname()))\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger \u001b[39m=\u001b[39m logger\n\u001b[1;32m     81\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m     82\u001b[0m     \u001b[39m'\u001b[39m\u001b[39minitializing backend \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m     83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname(), \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mversion()))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: espeak not installed on your system"
     ]
    }
   ],
   "source": [
    "from text import tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "text_norm = data[\"normalized_text\"].tolist()\n",
    "for cleaners in text_cleaners:\n",
    "    print(f\"Cleaning with {cleaners} ...\")\n",
    "    if cleaners[0] == \"phonemize_text\":\n",
    "        text_norm = tokenizer(text_norm, Vocab, cleaners, language=hps.data.language)\n",
    "    else:\n",
    "        for idx, text in enumerate(text_norm):\n",
    "            temp = tokenizer(text, Vocab, cleaners, language=hps.data.language)\n",
    "            text_norm[idx] = temp\n",
    "\n",
    "data = data.assign(cleaned_text=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39myield\u001b[39;00m text\u001b[39m.\u001b[39msplit()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m text_norm \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mcleaned_text\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m vocab \u001b[39m=\u001b[39m build_vocab_from_iterator(yield_tokens(text_norm), specials\u001b[39m=\u001b[39;49mspecial_symbols)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m vocab\u001b[39m.\u001b[39mset_default_index(UNK_ID)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m vocab_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../vocab.txt\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vits2/lib/python3.11/site-packages/torchtext/vocab/vocab_factory.py:98\u001b[0m, in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first, max_tokens)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mBuild a Vocab from an iterator.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m    >>> vocab = build_vocab_from_iterator(yield_tokens(file_path), specials=[\"<unk>\"])\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m counter \u001b[39m=\u001b[39m Counter()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mfor\u001b[39;49;00m tokens \u001b[39min\u001b[39;49;00m iterator:\n\u001b[1;32m     99\u001b[0m     counter\u001b[39m.\u001b[39;49mupdate(tokens)\n\u001b[1;32m    101\u001b[0m specials \u001b[39m=\u001b[39m specials \u001b[39mor\u001b[39;00m []\n",
      "\u001b[1;32m/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb Cell 11\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39myield_tokens\u001b[39m(cleaned_text: List[\u001b[39mstr\u001b[39m]):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m cleaned_text:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/henrytse/code/VitsTraining/AI_Vtuber_VoiceTraining-1/vits2/datasets/custom_base/prepare/filelists.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39myield\u001b[39;00m text\u001b[39m.\u001b[39;49msplit()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from utils.task import load_vocab, save_vocab\n",
    "from text.symbols import special_symbols, UNK_ID\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def yield_tokens(cleaned_text: List[str]):\n",
    "    for text in cleaned_text:\n",
    "        yield text.split()\n",
    "\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "vocab = build_vocab_from_iterator(yield_tokens(text_norm), specials=special_symbols)\n",
    "vocab.set_default_index(UNK_ID)\n",
    "\n",
    "vocab_file = f\"../vocab.txt\"\n",
    "save_vocab(vocab, vocab_file)\n",
    "\n",
    "vocab = load_vocab(vocab_file)\n",
    "print(f\"Size of vocabulary: {len(vocab)}\")\n",
    "print(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token cleaners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DUMMY1/LJ001-0001.wav</td>\n",
       "      <td>Printing, in the only sense with which we are ...</td>\n",
       "      <td>Printing, in the only sense with which we are ...</td>\n",
       "      <td>p ɹ ˈɪ n t ɪ ŋ , &lt;space&gt; ˈɪ n &lt;space&gt; ð ə &lt;spa...</td>\n",
       "      <td>2\\t19\\t12\\t18\\t6\\t7\\t15\\t42\\t27\\t4\\t18\\t6\\t4\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DUMMY1/LJ001-0002.wav</td>\n",
       "      <td>in being comparatively modern.</td>\n",
       "      <td>in being comparatively modern.</td>\n",
       "      <td>ˈɪ n &lt;space&gt; b ˈiː ɪ ŋ &lt;space&gt; k ə m p ˈæ ɹ ə ...</td>\n",
       "      <td>2\\t18\\t6\\t4\\t25\\t36\\t15\\t42\\t4\\t13\\t8\\t17\\t19\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DUMMY1/LJ001-0003.wav</td>\n",
       "      <td>For although the Chinese took impressions from...</td>\n",
       "      <td>For although the Chinese took impressions from...</td>\n",
       "      <td>f ɔːɹ &lt;space&gt; ɔː l ð ˈoʊ &lt;space&gt; ð ə &lt;space&gt; t...</td>\n",
       "      <td>2\\t23\\t59\\t4\\t92\\t16\\t11\\t39\\t4\\t11\\t8\\t4\\t50\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DUMMY1/LJ001-0004.wav</td>\n",
       "      <td>produced the block books, which were the immed...</td>\n",
       "      <td>produced the block books, which were the immed...</td>\n",
       "      <td>p ɹ ə d ˈuː s t &lt;space&gt; ð ə &lt;space&gt; b l ˈɑː k ...</td>\n",
       "      <td>2\\t19\\t12\\t8\\t10\\t44\\t9\\t7\\t4\\t11\\t8\\t4\\t25\\t1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DUMMY1/LJ001-0005.wav</td>\n",
       "      <td>the invention of movable metal letters in the ...</td>\n",
       "      <td>the invention of movable metal letters in the ...</td>\n",
       "      <td>ð ə &lt;space&gt; ɪ n v ˈɛ n ʃ ə n &lt;space&gt; ʌ v &lt;spac...</td>\n",
       "      <td>2\\t11\\t8\\t4\\t15\\t6\\t21\\t22\\t6\\t37\\t8\\t6\\t4\\t28...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file                                               text  \\\n",
       "0  DUMMY1/LJ001-0001.wav  Printing, in the only sense with which we are ...   \n",
       "1  DUMMY1/LJ001-0002.wav                     in being comparatively modern.   \n",
       "2  DUMMY1/LJ001-0003.wav  For although the Chinese took impressions from...   \n",
       "3  DUMMY1/LJ001-0004.wav  produced the block books, which were the immed...   \n",
       "4  DUMMY1/LJ001-0005.wav  the invention of movable metal letters in the ...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  Printing, in the only sense with which we are ...   \n",
       "1                     in being comparatively modern.   \n",
       "2  For although the Chinese took impressions from...   \n",
       "3  produced the block books, which were the immed...   \n",
       "4  the invention of movable metal letters in the ...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  p ɹ ˈɪ n t ɪ ŋ , <space> ˈɪ n <space> ð ə <spa...   \n",
       "1  ˈɪ n <space> b ˈiː ɪ ŋ <space> k ə m p ˈæ ɹ ə ...   \n",
       "2  f ɔːɹ <space> ɔː l ð ˈoʊ <space> ð ə <space> t...   \n",
       "3  p ɹ ə d ˈuː s t <space> ð ə <space> b l ˈɑː k ...   \n",
       "4  ð ə <space> ɪ n v ˈɛ n ʃ ə n <space> ʌ v <spac...   \n",
       "\n",
       "                                              tokens  \n",
       "0  2\\t19\\t12\\t18\\t6\\t7\\t15\\t42\\t27\\t4\\t18\\t6\\t4\\t...  \n",
       "1  2\\t18\\t6\\t4\\t25\\t36\\t15\\t42\\t4\\t13\\t8\\t17\\t19\\...  \n",
       "2  2\\t23\\t59\\t4\\t92\\t16\\t11\\t39\\t4\\t11\\t8\\t4\\t50\\...  \n",
       "3  2\\t19\\t12\\t8\\t10\\t44\\t9\\t7\\t4\\t11\\t8\\t4\\t25\\t1...  \n",
       "4  2\\t11\\t8\\t4\\t15\\t6\\t21\\t22\\t6\\t37\\t8\\t6\\t4\\t28...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from text import detokenizer\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "for idx, text in enumerate(text_norm):\n",
    "    temp = tokenizer(text, vocab, token_cleaners, language=hps.data.language)\n",
    "    assert UNK_ID not in temp, f\"Found unknown symbol:\\n{text}\\n{detokenizer(temp)}\"\n",
    "    text_norm[idx] = temp\n",
    "\n",
    "text_norm = [\"\\t\".join(map(str, text)) for text in text_norm]\n",
    "data = data.assign(tokens=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save train, val, test filelists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"file\", \"tokens\"]]\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data_train = data.iloc[n_val + n_test:]\n",
    "data_val = data.iloc[:n_val]\n",
    "data_test = data.iloc[n_val: n_val + n_test]\n",
    "\n",
    "data_train.to_csv(\"../filelists/train.txt\", sep=\"|\", index=False, header=False)\n",
    "data_val.to_csv(\"../filelists/val.txt\", sep=\"|\", index=False, header=False)\n",
    "data_test.to_csv(\"../filelists/test.txt\", sep=\"|\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naturalspeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
